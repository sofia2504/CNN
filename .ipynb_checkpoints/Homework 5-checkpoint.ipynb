{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68405135",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 18px;\">Question 1</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d429a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bbb83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Data loading and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0482fc",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 18px;\">Question 2</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64593dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different MLP architectures\n",
    "class MLP1(nn.Module):\n",
    "    def __init__(self, activation=nn.ReLU()):\n",
    "        super(MLP1, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    def __init__(self, activation=nn.ReLU()):\n",
    "        super(MLP2, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 10)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class MLP3(nn.Module):\n",
    "    def __init__(self, activation=nn.ReLU()):\n",
    "        super(MLP3, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class MLP4(nn.Module):\n",
    "    def __init__(self, activation=nn.ReLU()):\n",
    "        super(MLP4, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class MLP5(nn.Module):\n",
    "    def __init__(self, activation=nn.ReLU()):\n",
    "        super(MLP5, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 10)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.activation(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbe95311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, epochs=10, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        test_accuracy = evaluate_model(model, testloader, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Training Loss: {train_loss:.4f}')\n",
    "        print(f'Training Accuracy: {train_accuracy:.2f}%')\n",
    "        print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "        print('--------------------')\n",
    "    \n",
    "    return train_losses, train_accuracies, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d01c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "075184de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP1 (3 layers)\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.4813\n",
      "Training Accuracy: 82.32%\n",
      "Test Accuracy: 84.22%\n",
      "--------------------\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.3682\n",
      "Training Accuracy: 86.44%\n",
      "Test Accuracy: 86.09%\n",
      "--------------------\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.3275\n",
      "Training Accuracy: 87.83%\n",
      "Test Accuracy: 86.56%\n",
      "--------------------\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.3015\n",
      "Training Accuracy: 88.78%\n",
      "Test Accuracy: 87.14%\n",
      "--------------------\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.2819\n",
      "Training Accuracy: 89.39%\n",
      "Test Accuracy: 87.87%\n",
      "--------------------\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.2650\n",
      "Training Accuracy: 90.09%\n",
      "Test Accuracy: 87.14%\n",
      "--------------------\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.2502\n",
      "Training Accuracy: 90.52%\n",
      "Test Accuracy: 87.63%\n",
      "--------------------\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.2387\n",
      "Training Accuracy: 91.03%\n",
      "Test Accuracy: 87.42%\n",
      "--------------------\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.2279\n",
      "Training Accuracy: 91.40%\n",
      "Test Accuracy: 88.50%\n",
      "--------------------\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.2171\n",
      "Training Accuracy: 91.82%\n",
      "Test Accuracy: 88.00%\n",
      "--------------------\n",
      "\n",
      "Training MLP2 (4 layers)\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.5066\n",
      "Training Accuracy: 81.43%\n",
      "Test Accuracy: 84.90%\n",
      "--------------------\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.3863\n",
      "Training Accuracy: 85.88%\n",
      "Test Accuracy: 85.93%\n",
      "--------------------\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.3379\n",
      "Training Accuracy: 87.50%\n",
      "Test Accuracy: 84.55%\n",
      "--------------------\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.3129\n",
      "Training Accuracy: 88.42%\n",
      "Test Accuracy: 87.03%\n",
      "--------------------\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.2922\n",
      "Training Accuracy: 89.17%\n",
      "Test Accuracy: 87.12%\n",
      "--------------------\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.2766\n",
      "Training Accuracy: 89.69%\n",
      "Test Accuracy: 86.64%\n",
      "--------------------\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.2610\n",
      "Training Accuracy: 90.29%\n",
      "Test Accuracy: 88.01%\n",
      "--------------------\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.2473\n",
      "Training Accuracy: 90.67%\n",
      "Test Accuracy: 87.44%\n",
      "--------------------\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.2367\n",
      "Training Accuracy: 91.15%\n",
      "Test Accuracy: 88.48%\n",
      "--------------------\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.2265\n",
      "Training Accuracy: 91.41%\n",
      "Test Accuracy: 87.68%\n",
      "--------------------\n",
      "\n",
      "Training MLP3 (3 layers, smaller)\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.4844\n",
      "Training Accuracy: 82.31%\n",
      "Test Accuracy: 84.50%\n",
      "--------------------\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.3689\n",
      "Training Accuracy: 86.43%\n",
      "Test Accuracy: 84.91%\n",
      "--------------------\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.3288\n",
      "Training Accuracy: 87.76%\n",
      "Test Accuracy: 86.79%\n",
      "--------------------\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.3046\n",
      "Training Accuracy: 88.73%\n",
      "Test Accuracy: 86.55%\n",
      "--------------------\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.2859\n",
      "Training Accuracy: 89.23%\n",
      "Test Accuracy: 87.36%\n",
      "--------------------\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.2677\n",
      "Training Accuracy: 90.00%\n",
      "Test Accuracy: 87.45%\n",
      "--------------------\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.2524\n",
      "Training Accuracy: 90.51%\n",
      "Test Accuracy: 87.74%\n",
      "--------------------\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.2434\n",
      "Training Accuracy: 90.91%\n",
      "Test Accuracy: 87.48%\n",
      "--------------------\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.2335\n",
      "Training Accuracy: 91.21%\n",
      "Test Accuracy: 88.28%\n",
      "--------------------\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.2245\n",
      "Training Accuracy: 91.51%\n",
      "Test Accuracy: 88.09%\n",
      "--------------------\n",
      "\n",
      "Training MLP4 (2 layers)\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.4770\n",
      "Training Accuracy: 82.45%\n",
      "Test Accuracy: 84.32%\n",
      "--------------------\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.3706\n",
      "Training Accuracy: 86.29%\n",
      "Test Accuracy: 85.18%\n",
      "--------------------\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.3329\n",
      "Training Accuracy: 87.71%\n",
      "Test Accuracy: 86.53%\n",
      "--------------------\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.3054\n",
      "Training Accuracy: 88.71%\n",
      "Test Accuracy: 86.29%\n",
      "--------------------\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.2873\n",
      "Training Accuracy: 89.41%\n",
      "Test Accuracy: 87.87%\n",
      "--------------------\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.2721\n",
      "Training Accuracy: 89.85%\n",
      "Test Accuracy: 87.70%\n",
      "--------------------\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.2592\n",
      "Training Accuracy: 90.41%\n",
      "Test Accuracy: 88.21%\n",
      "--------------------\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.2483\n",
      "Training Accuracy: 90.84%\n",
      "Test Accuracy: 88.21%\n",
      "--------------------\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.2355\n",
      "Training Accuracy: 91.27%\n",
      "Test Accuracy: 87.50%\n",
      "--------------------\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.2278\n",
      "Training Accuracy: 91.47%\n",
      "Test Accuracy: 88.35%\n",
      "--------------------\n",
      "\n",
      "Training MLP5 (5 layers)\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.5275\n",
      "Training Accuracy: 80.56%\n",
      "Test Accuracy: 83.77%\n",
      "--------------------\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.3933\n",
      "Training Accuracy: 85.97%\n",
      "Test Accuracy: 84.22%\n",
      "--------------------\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.3495\n",
      "Training Accuracy: 87.25%\n",
      "Test Accuracy: 86.36%\n",
      "--------------------\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.3195\n",
      "Training Accuracy: 88.39%\n",
      "Test Accuracy: 86.86%\n",
      "--------------------\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.3017\n",
      "Training Accuracy: 88.88%\n",
      "Test Accuracy: 86.99%\n",
      "--------------------\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.2829\n",
      "Training Accuracy: 89.55%\n",
      "Test Accuracy: 87.81%\n",
      "--------------------\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.2695\n",
      "Training Accuracy: 90.05%\n",
      "Test Accuracy: 87.50%\n",
      "--------------------\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.2561\n",
      "Training Accuracy: 90.61%\n",
      "Test Accuracy: 87.94%\n",
      "--------------------\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.2451\n",
      "Training Accuracy: 91.01%\n",
      "Test Accuracy: 88.03%\n",
      "--------------------\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.2354\n",
      "Training Accuracy: 91.25%\n",
      "Test Accuracy: 87.44%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Train different MLP architectures\n",
    "mlp_architectures = [\n",
    "    (\"MLP1 (3 layers)\", MLP1()),\n",
    "    (\"MLP2 (4 layers)\", MLP2()),\n",
    "    (\"MLP3 (3 layers, smaller)\", MLP3()),\n",
    "    (\"MLP4 (2 layers)\", MLP4()),\n",
    "    (\"MLP5 (5 layers)\", MLP5())\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for name, model in mlp_architectures:\n",
    "    print(f\"\\nTraining {name}\")\n",
    "    train_losses, train_accuracies, test_accuracies = train_model(model)\n",
    "    results[name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'test_accuracies': test_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1630529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training best architecture with ReLU activation\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.4781\n",
      "Training Accuracy: 82.44%\n",
      "Test Accuracy: 83.24%\n",
      "--------------------\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.3670\n",
      "Training Accuracy: 86.66%\n",
      "Test Accuracy: 85.62%\n",
      "--------------------\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.3312\n",
      "Training Accuracy: 87.77%\n",
      "Test Accuracy: 85.92%\n",
      "--------------------\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.3056\n",
      "Training Accuracy: 88.68%\n",
      "Test Accuracy: 84.24%\n",
      "--------------------\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.2887\n",
      "Training Accuracy: 89.22%\n",
      "Test Accuracy: 87.69%\n",
      "--------------------\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.2710\n",
      "Training Accuracy: 89.81%\n",
      "Test Accuracy: 86.83%\n",
      "--------------------\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.2569\n",
      "Training Accuracy: 90.44%\n",
      "Test Accuracy: 87.66%\n",
      "--------------------\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.2469\n",
      "Training Accuracy: 90.73%\n",
      "Test Accuracy: 87.95%\n",
      "--------------------\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.2378\n",
      "Training Accuracy: 91.14%\n",
      "Test Accuracy: 88.06%\n",
      "--------------------\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.2235\n",
      "Training Accuracy: 91.62%\n",
      "Test Accuracy: 87.53%\n",
      "--------------------\n",
      "\n",
      "Training best architecture with Tanh activation\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.4866\n",
      "Training Accuracy: 82.36%\n",
      "Test Accuracy: 83.57%\n",
      "--------------------\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.3942\n",
      "Training Accuracy: 85.68%\n",
      "Test Accuracy: 84.75%\n",
      "--------------------\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.3717\n",
      "Training Accuracy: 86.38%\n",
      "Test Accuracy: 83.70%\n",
      "--------------------\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.3560\n",
      "Training Accuracy: 87.02%\n",
      "Test Accuracy: 86.04%\n",
      "--------------------\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.3413\n",
      "Training Accuracy: 87.47%\n",
      "Test Accuracy: 86.52%\n",
      "--------------------\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.3278\n",
      "Training Accuracy: 87.90%\n",
      "Test Accuracy: 86.24%\n",
      "--------------------\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.3209\n",
      "Training Accuracy: 88.27%\n",
      "Test Accuracy: 85.69%\n",
      "--------------------\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.3094\n",
      "Training Accuracy: 88.59%\n",
      "Test Accuracy: 86.56%\n",
      "--------------------\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.3120\n",
      "Training Accuracy: 88.70%\n",
      "Test Accuracy: 86.28%\n",
      "--------------------\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.2982\n",
      "Training Accuracy: 89.17%\n",
      "Test Accuracy: 86.71%\n",
      "--------------------\n",
      "\n",
      "Training best architecture with Sigmoid activation\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.4993\n",
      "Training Accuracy: 82.00%\n",
      "Test Accuracy: 83.31%\n",
      "--------------------\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.3721\n",
      "Training Accuracy: 86.47%\n",
      "Test Accuracy: 85.92%\n",
      "--------------------\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.3314\n",
      "Training Accuracy: 87.84%\n",
      "Test Accuracy: 86.72%\n",
      "--------------------\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.3054\n",
      "Training Accuracy: 88.71%\n",
      "Test Accuracy: 87.00%\n",
      "--------------------\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.2840\n",
      "Training Accuracy: 89.54%\n",
      "Test Accuracy: 87.16%\n",
      "--------------------\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.2697\n",
      "Training Accuracy: 89.96%\n",
      "Test Accuracy: 87.54%\n",
      "--------------------\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.2534\n",
      "Training Accuracy: 90.58%\n",
      "Test Accuracy: 87.71%\n",
      "--------------------\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.2411\n",
      "Training Accuracy: 91.01%\n",
      "Test Accuracy: 87.55%\n",
      "--------------------\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.2287\n",
      "Training Accuracy: 91.53%\n",
      "Test Accuracy: 88.21%\n",
      "--------------------\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.2185\n",
      "Training Accuracy: 91.83%\n",
      "Test Accuracy: 87.84%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#Test different activation functions on the best performing architecture\n",
    "best_architecture = MLP4\n",
    "activation_functions = [\n",
    "    (\"ReLU\", nn.ReLU()),\n",
    "    (\"Tanh\", nn.Tanh()),\n",
    "    (\"Sigmoid\", nn.Sigmoid())\n",
    "]\n",
    "\n",
    "activation_results = {}\n",
    "for name, activation in activation_functions:\n",
    "    print(f\"\\nTraining best architecture with {name} activation\")\n",
    "    model = best_architecture(activation=activation)\n",
    "    train_losses, train_accuracies, test_accuracies = train_model(model)\n",
    "    activation_results[name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'test_accuracies': test_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions for visualization\n",
    "def plot_results(results, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, data in results.items():\n",
    "        plt.plot(data['train_accuracies'], label=f\"{name} (Train)\")\n",
    "        plt.plot(data['test_accuracies'], label=f\"{name} (Test)\")\n",
    "    plt.title('Training and Test Accuracies')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name, data in results.items():\n",
    "        plt.plot(data['train_losses'], label=name)\n",
    "    plt.title('Training Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results\n",
    "plot_results(results, \"Comparison of Different MLP Architectures\")\n",
    "plot_results(activation_results, \"Comparison of Different Activation Functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a41de",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 18px;\">Question 3</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1f4d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base CNN architecture\n",
    "class BaseCNN(nn.Module):\n",
    "    def __init__(self, dropout=False):\n",
    "        super(BaseCNN, self).__init__()\n",
    "        # First convolutional layer with 32 3x3 filters a\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        # MaxPool layer with 2x2 window b\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Calculate size after conv and poolc\n",
    "        self.dropout = dropout\n",
    "        if dropout:\n",
    "            self.dropout_layer = nn.Dropout(0.5)\n",
    "        # Dense layers d\n",
    "        self.fc1 = nn.Linear(32 * 13 * 13, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 32 * 13 * 13)\n",
    "        if self.dropout:\n",
    "            x = self.dropout_layer(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb1af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced CNN with two conv layers\n",
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self, dropout=True):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = dropout\n",
    "        if dropout:\n",
    "            self.dropout_layer = nn.Dropout(0.5)\n",
    "        # Adjusted size for two conv layers\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        if self.dropout:\n",
    "            x = self.dropout_layer(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "499f2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, testloader, epochs, lr=0.01, momentum=0.9):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        train_accuracy = 100 * correct / total\n",
    "        test_accuracy = evaluate_model(model, testloader, device)\n",
    "        \n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}:')\n",
    "            print(f'Training Accuracy: {train_accuracy:.2f}%')\n",
    "            print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "            print('--------------------')\n",
    "    \n",
    "    return train_accuracies, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a85ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77bba8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(train_acc, test_acc, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = len(train_acc)\n",
    "    x = list(range(1, epochs + 1))\n",
    "    plt.plot(x, train_acc, label='Training Accuracy')\n",
    "    plt.plot(x, test_acc, label='Test Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5742d367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Baseline CNN for 10 epochs:\n",
      "Epoch 10/10:\n",
      "Training Accuracy: 96.50%\n",
      "Test Accuracy: 91.46%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Baseline CNN - 10 epochs\n",
    "print(\"Training Baseline CNN for 10 epochs:\")\n",
    "model = BaseCNN()\n",
    "train_acc, test_acc = train_model(model, trainloader, testloader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7788c763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CNN with Dropout for 50 epochs:\n",
      "Epoch 10/50:\n",
      "Training Accuracy: 92.57%\n",
      "Test Accuracy: 90.73%\n",
      "--------------------\n",
      "Epoch 20/50:\n",
      "Training Accuracy: 94.36%\n",
      "Test Accuracy: 90.86%\n",
      "--------------------\n",
      "Epoch 30/50:\n",
      "Training Accuracy: 95.47%\n",
      "Test Accuracy: 91.47%\n",
      "--------------------\n",
      "Epoch 40/50:\n",
      "Training Accuracy: 96.19%\n",
      "Test Accuracy: 91.28%\n",
      "--------------------\n",
      "Epoch 50/50:\n",
      "Training Accuracy: 96.70%\n",
      "Test Accuracy: 91.55%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# CNN with Dropout - 50 epochs\n",
    "print(\"\\nTraining CNN with Dropout for 50 epochs:\")\n",
    "model_dropout = BaseCNN(dropout=True)\n",
    "train_acc_dropout, test_acc_dropout = train_model(model_dropout, trainloader, testloader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac4c7ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_accuracies(train_acc_dropout, test_acc_dropout, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN with Dropout - Training vs Test Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "plot_accuracies(train_acc_dropout, test_acc_dropout, 'CNN with Dropout - Training vs Test Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
